{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8739946c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mglearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmglearn\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mglearn'"
     ]
    }
   ],
   "source": [
    "import mglearn\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c9a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_linear_regression_wave()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a12243",
   "metadata": {},
   "source": [
    "**It can be observed that the slope is about 0.4(to figure this out Y/X) and the intercept is a tad bit below 0**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fabe16",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "- Linear Regression upsets the value of the Coefficient of feature/(slope) and the y-intercept that minimizes the mean square error between the predicted y value and the true regression target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d799ee3",
   "metadata": {},
   "source": [
    "### Ordinary least squares/ Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa708c27",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mglearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 3\u001b[0m X,y \u001b[38;5;241m=\u001b[39m \u001b[43mmglearn\u001b[49m\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mmake_wave(n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m      4\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X,y, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      5\u001b[0m lr \u001b[38;5;241m=\u001b[39m LinearRegression()\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mglearn' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X,y = mglearn.datasets.make_wave(n_samples=60)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 5)\n",
    "lr = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60850a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"lr.cofficient:{}\".format(lr.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc42e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"lr.intercept:{}\". format(lr.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a14f93",
   "metadata": {},
   "source": [
    "**Sklearn stores any thing derived from a training data with and underscore to create a seperation from the values that may be later set by individuals themselves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f280139",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score:{}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test set score:{}\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0059dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6510914",
   "metadata": {},
   "source": [
    "#### Trying the same model on the boston housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1720eb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mglearn.datasets.load_extended_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 6)\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "print(\"Train set score:{}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test set score:{}\".format(lr.score(X_test, y_test))) #Overfitted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fee101a",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab202ea8",
   "metadata": {},
   "source": [
    "**Very similar to ordinary least squares but it has a fixed coefficent which tends to zero making the features have very little effect on the prediction.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f5f11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "#we continued with the boston data set\n",
    "#default alpha parameter used is 1.0 \n",
    "ridge = Ridge().fit(X_train, y_train)\n",
    " \n",
    "print(\"Training set score:{:.2}\".format(ridge.score(X_train, y_train)))\n",
    "print(\"Test set score:{:.2}\".format(ridge.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6498b5",
   "metadata": {},
   "source": [
    "- Increasing alpha makes the coefficient tend to zero which results to the model making a more generalized prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39597ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A more generalized model\n",
    "ridge_10 = Ridge(alpha = 10).fit(X_train, y_train)\n",
    "print(\"Training set score:{:.2}\".format(ridge_10.score(X_train, y_train)))\n",
    "print(\"Test set score:{:.2}\".format(ridge_10.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6513e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A model with higher training set performance\n",
    "#reduced alpha, less restrictions similar model to the odinary least square model\n",
    "ridge_01 = Ridge(alpha = 0.1).fit(X_train, y_train)\n",
    "print(\"Training set score:{}\".format(ridge_01.score(X_train, y_train)))\n",
    "print(\"Test set score:{}\".format(ridge_01.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ef5edb",
   "metadata": {},
   "source": [
    "- A reduction of the alpha resulted to a higher performance on the trining set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeee368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3062e27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ridge.coef_, \"s\", label = \"Ridge_alpha = 1\")\n",
    "plt.plot(ridge_10.coef_, \"^\", label = \"Ridge_alpha = 10\")\n",
    "plt.plot(ridge_01.coef_, \"v\", label = \"Ridge_alpha = 0.1\")\n",
    "\n",
    "plt.plot(lr.coef_, \"o\", label = \"LinearRegression\")\n",
    "\n",
    "plt.xlabel(\"coefficient index\")\n",
    "plt.ylabel(\"coefficient magnitude\")\n",
    "plt.hlines(0,0,len(lr.coef_))\n",
    "plt.ylim(-50,50)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d22adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning curves for ridge regression and linear regression on the Boston Housing dataset\n",
    "mglearn.plots.plot_ridge_n_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23708da0",
   "metadata": {},
   "source": [
    "It is observed from the graph above that linear regression didnt perform at all before the late 300s, and had a quick learning rate and caught up to the Ridge model. This means that the Ridge Model can perform decently well on small samples unlike the Linear Regression Model(OLS).\n",
    "\n",
    "Also, the ridge training set dipped a bit during evaluation this is because as the model increases in size it becomes difficult for the model to overfit meaning the model doesnt memorize the data.\n",
    "With more data regularization is not so important.\n",
    "\n",
    "Regularization: resitricting the model to prevent memorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1342a896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee7ccef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed667f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9b55ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a81d70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4605c05e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
